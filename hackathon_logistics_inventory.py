# -*- coding: utf-8 -*-
"""Hackathon_Logistics_Inventory.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16jTRy5gmavQWTq-4AReM45JHkTomhLpz
"""

pip install category_encoders

# Commented out IPython magic to ensure Python compatibility.
#import the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import confusion_matrix, classification_report, f1_score
import category_encoders as ce

#importing the required datasets into colab
from google.colab import files
uploaded = files.upload()

df_train = pd.read_csv('Train_Set.csv')
df_train.head()

#importing the required datasets (test file) into colab
from google.colab import files
uploaded = files.upload()

df_test = pd.read_csv('Test_Set.csv')
df_test.head()

#check shape of the datasets
print(df_train.shape)
print(df_test.shape)

#check the info of both datasets
df_train.info()

df_test.info()

#get summary statistics on the train set
df_train.describe()

#get summary statistics on test set
df_test.describe()

#check missing values on train data
df_train.isnull().sum()

#check missing values as a percentage of the data
df_train.isnull().sum() / len(df_train)*100

#the columns sales, discount, product length, product weight, delivery review and session ordered have 2.86%, 4%, 2.33%, 0.55%, 7.17% and 3.79% 
#missing values respectively in the train data

#check missing values on test data
df_test.isnull().sum() / len(df_test)*100

#the columns sales, discount, product length, product weight, delivery review and session ordered have 2.69%, 4%, 2.4%, 0.56%, 7.19% and 3.75% 
#missing values respectively in the test data

#visualize missing values using heatmap on both datasets
plt.figure(figsize=(15,8))
sns.heatmap(data=df_train.isnull(), cbar=False)

plt.figure(figsize=(15,8))
sns.heatmap(data=df_test.isnull(), cbar=False)

"""The columns having missing values in both the train and test data are the same and the proportion of missing values is also almost equal in both datasets.
We need to impute the missing values in both sets of data by a common method.
"""



#check the distribution of the sales column in train data
sns.displot(df_train['Sales'])

#looking at the distribution plot, sales of around 200 per customer has the highest count

#check for test data
sns.displot(df_test['Sales'])

#The distribution is almost same as on the training data

df_train.describe()['Sales']

df_test.describe()['Sales']

"""Looking at the summary statistics for the Sales column in both data sets, we have mean and median ranges as 195 and 173 respectively."""

#the mean and median ranges are not very close to each other. Let us impute by the median values

#make copy of train set and impute on this data
df_train_copy = df_train.copy()

#make copy of test set and impute on this data
df_test_copy = df_test.copy()

#use median to impute the missing values on train set
df_train_copy['Sales'] = df_train_copy['Sales'].fillna(df_train_copy['Sales'].median())

df_train_copy.isnull().sum()

#use median to impute the missing values on test set
df_test_copy['Sales'] = df_test_copy['Sales'].fillna(df_test_copy['Sales'].median())

df_test_copy.isnull().sum()

#check summary statistics for Sales column after imputation
df_train_copy.describe()['Sales']

df_test_copy.describe()['Sales']

#check distrbution and summary statistics for discount column on train and test sets
sns.displot(df_train['Discount'])

df_train.describe()['Discount']

sns.displot(df_test['Discount'])

df_test.describe()['Discount']

df_train['Discount'].value_counts()

df_test['Discount'].value_counts()

"""The column Discount is a categorical feature. We will again use the knn imputer for imputing the missing values in this column."""

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df_train_copy['Discount'] = imputer.fit_transform(df_train_copy[['Discount']])

df_train_copy.isnull().sum()

#impute on test data
df_test_copy['Discount'] = imputer.fit_transform(df_test_copy[['Discount']])

df_test_copy.isnull().sum()

#check value counts
df_train_copy['Discount'].value_counts()

df_test_copy['Discount'].value_counts()

#check the summary statistics and distribution for product length
df_train_copy.describe()['ProductLength']

df_test_copy.describe()['ProductLength']

#the mean and median values are not close to each other

#check distribution on both sets
sns.displot(df_train_copy['ProductLength'])

sns.displot(df_test_copy['ProductLength'])

#we have almost similar distribution on both data sets. We'll again use the median value to impute the missing values

#impute missing data on train set
df_train_copy['ProductLength'] = df_train_copy['ProductLength'].fillna(df_train_copy['ProductLength'].median())

df_train_copy['ProductLength'].isnull().sum()

#impute on test data
df_test_copy['ProductLength'] = df_test_copy['ProductLength'].fillna(df_test_copy['ProductLength'].median())

df_test_copy['ProductLength'].isnull().sum()

#impute missing values in the product weight column as done in the productlength column by median

#impute missing data on train set
df_train_copy['ProductWeight'] = df_train_copy['ProductWeight'].fillna(df_train_copy['ProductWeight'].median())

df_train_copy['ProductWeight'].isnull().sum()

#impute the missing values on the test data
df_test_copy['ProductWeight'] = df_test_copy['ProductWeight'].fillna(df_test_copy['ProductWeight'].median())

df_test_copy['ProductWeight'].isnull().sum()

df_train_copy.isnull().sum()

df_test_copy.isnull().sum()

#treat the missing values in Delivery_review column
df_train_copy['Delivery_Review'].value_counts().plot(kind='bar')

df_test_copy['Delivery_Review'].value_counts().plot(kind='bar')

#we can see that both plots have almost same distribution of values.

#check summary statistics
df_train_copy.describe()['Delivery_Review']

#the mean and median values are almost same i.e 3

df_test_copy.describe()['Delivery_Review']

#same observation recorded for test set

#Here, we can replace the missing values by the value 3 in both train and test data

df_train_copy['Delivery_Review'] = df_train_copy['Delivery_Review'].replace(np.nan, 3)

df_train_copy['Delivery_Review'].isnull().sum()

df_train_copy['Delivery_Review'].value_counts()

#imputing on test data
df_test_copy['Delivery_Review'] = df_test_copy['Delivery_Review'].replace(np.nan, 3)

df_test_copy['Delivery_Review'].value_counts()

df_test_copy['Delivery_Review'].isnull().sum()

#check missing data in the column session ordered
df_train_copy['Session_Ordered'].value_counts()

df_train_copy['Session_Ordered'].describe()

#here mean and median values are same. ie 2. We will impute the missing values with 2
df_train_copy['Session_Ordered'] = df_train_copy['Session_Ordered'].replace(np.nan, 2)

df_train_copy['Session_Ordered'].isnull().sum()

df_train_copy['Session_Ordered'].value_counts()

#impute on the test data
df_test_copy['Session_Ordered'].value_counts()

df_test_copy['Session_Ordered'].describe()

#the mean and median values are almost same i.e 2. Impute with 2

df_test_copy['Session_Ordered'] = df_test_copy['Session_Ordered'].replace(np.nan, 2)

df_test_copy['Session_Ordered'].isnull().sum()

df_test_copy['Session_Ordered'].value_counts()

#check missing values for entire datasets
df_train_copy.isnull().sum()

df_test_copy.isnull().sum()

"""All the missing values for both train and test sets have been handled by using imputation techniques."""



#check correlation matrix for all variables in both datasets using heatmap
plt.figure(figsize=(15,8))
sns.heatmap(data=df_train_copy.corr(), annot=False)
plt.show()

plt.figure(figsize=(15,8))
sns.heatmap(data=df_test_copy.corr(), annot=False)
plt.show()

#there is no significant relationship found between the variables except for some categorical variables like prod_cat-id and dept_id.

"""Outlier detection and Treatment"""

#for train data
plt.figure(figsize=(15,8))
plt.tight_layout()
df_train_copy.boxplot()
plt.xticks(rotation=90)
plt.show()

#for test data
plt.figure(figsize=(15,8))
plt.tight_layout()
df_test_copy.boxplot()
plt.xticks(rotation=90)
plt.show()

"""We can observe that columns cust_id, sales, price, dept_id, profit_ratio, order_profit, product weight and delivery review have outlier values.

Out of these columns, we can ignore columns cust_id, dept_id and delivery review. Id columns are not required and review column is categorical.
"""

#Boxplot of sales
plt.figure(figsize=(15,8))
sns.boxplot(df_train_copy['Sales'])
plt.show()

df_train_copy['Sales'].describe()

df_train_copy[df_train_copy['Sales'] > 600].shape[0]

#We can cap the outlier values to the value 600.

#capping the outliers on the train data
df_train_copy['Sales'] = np.where(df_train_copy['Sales'] > 600.0, 600.0, df_train_copy['Sales'])

df_train_copy[df_train_copy['Sales'] > 600].shape[0]

#Boxplot of sales
plt.figure(figsize=(15,8))
sns.boxplot(df_test_copy['Sales'])
plt.show()

#capping the outliers on the test data
df_test_copy['Sales'] = np.where(df_test_copy['Sales'] > 600.0, 600.0, df_test_copy['Sales'])

df_test_copy[df_test_copy['Sales'] > 600].shape[0]

#outlier detection on price
#Boxplot of price on train data
plt.figure(figsize=(15,8))
sns.boxplot(df_train_copy['Price'])
plt.show()

#the data is heavily left skewed.

df_train_copy['Price'].describe()

df_train_copy[df_train_copy['Price'] > 30000].shape[0]

#cap the values to 30000
df_train_copy['Price'] = np.where(df_train_copy['Price'] > 30000.0, 30000.0, df_train_copy['Price'])

df_train_copy[df_train_copy['Price'] > 30000].shape[0]

#check on test set
#Boxplot of price on train data
plt.figure(figsize=(15,8))
sns.boxplot(df_test_copy['Price'])
plt.show()

#carry out same imputation as distribution is similar
df_test_copy['Price'] = np.where(df_test_copy['Price'] > 30000.0, 30000.0, df_test_copy['Price'])

df_test_copy[df_test_copy['Price'] > 30000].shape[0]

#profit_ratio
#check boxplot of profit ratio on the train data
plt.figure(figsize=(15,8))
sns.boxplot(df_train_copy['Profit_Ratio'])
plt.show()

df_train_copy['Profit_Ratio'].describe()

df_train_copy[df_train_copy['Profit_Ratio'] < -0.5].shape[0]

#capping the values to -0.5
df_train_copy['Profit_Ratio'] = np.where(df_train_copy['Profit_Ratio'] < -0.5, -0.5, df_train_copy['Profit_Ratio'])

df_train_copy[df_train_copy['Profit_Ratio'] < -0.5].shape[0]

#check the outlier distribution for profit ratio on test data
plt.figure(figsize=(15,8))
sns.boxplot(df_test_copy['Profit_Ratio'])
plt.show()

#Applying similar capping technique
df_test_copy['Profit_Ratio'] = np.where(df_test_copy['Profit_Ratio'] < -0.5, -0.5, df_test_copy['Profit_Ratio'])

df_test_copy[df_test_copy['Profit_Ratio'] < -0.5].shape[0]

#order profit column
plt.figure(figsize=(15,8))
sns.boxplot(df_train_copy['Order_Profit '])
plt.show()

df_train_copy['Order_Profit '].describe()

df_train_copy[df_train_copy['Order_Profit '] > 10000].shape[0]

df_train_copy[df_train_copy['Order_Profit '] < -10000].shape[0]

#capping outlier values to 10000 on both sides
df_train_copy['Order_Profit '] = np.where(df_train_copy['Order_Profit '] > 10000, 10000, df_train_copy['Order_Profit '])

df_train_copy[df_train_copy['Order_Profit '] > 10000].shape[0]

df_train_copy['Order_Profit '] = np.where(df_train_copy['Order_Profit '] < -10000, -10000, df_train_copy['Order_Profit '])

df_train_copy[df_train_copy['Order_Profit '] < -10000].shape[0]

#check on the test set
plt.figure(figsize=(15,8))
sns.boxplot(df_test_copy['Order_Profit '])
plt.show()

#similar distribution is observed so same capping technique for outlier values would be applied

df_test_copy[df_test_copy['Order_Profit '] > 10000].shape[0]

df_test_copy[df_test_copy['Order_Profit '] < -10000].shape[0]

df_test_copy['Order_Profit '] = np.where(df_test_copy['Order_Profit '] > 10000, 10000, df_test_copy['Order_Profit '])

df_test_copy['Order_Profit '] = np.where(df_test_copy['Order_Profit '] < -10000, -10000, df_test_copy['Order_Profit '])

df_test_copy[df_test_copy['Order_Profit '] > 10000].shape[0]

df_test_copy[df_test_copy['Order_Profit '] < -10000].shape[0]

#product weight
plt.figure(figsize=(15,8))
sns.boxplot(df_train_copy['ProductWeight'])
plt.show()

df_train_copy['ProductWeight'].describe()

df_train_copy[df_train_copy['ProductWeight'] > 4000].shape[0]

#cap the values to 4000
df_train_copy['ProductWeight'] = np.where(df_train_copy['ProductWeight'] > 4000, 4000, df_train_copy['ProductWeight'])

df_train_copy[df_train_copy['ProductWeight'] > 4000].shape[0]

#check distribution of outlier values on test set
plt.figure(figsize=(15,8))
sns.boxplot(df_test_copy['ProductWeight'])
plt.show()

#same distribution of values is observed

df_test_copy[df_test_copy['ProductWeight'] > 4000].shape[0]

df_test_copy['ProductWeight'].describe()

df_test_copy['ProductWeight'] = np.where(df_test_copy['ProductWeight'] > 4000, 4000, df_test_copy['ProductWeight'])

df_test_copy[df_test_copy['ProductWeight'] > 4000].shape[0]

#All the outliers have been now taken care of.

#Check distribution of our target variable
df_train_copy['Delivery_Status'].value_counts()
#0 : ontime
#1 : delayed

"""The ratio of target classes is approx 2:1. Seems that the target class is not well balanced"""

df_train_copy['Delivery_Status'].value_counts().plot(kind='bar')

df_train_cat = df_train_copy.select_dtypes(include='object')
df_train_cat.columns

#use binary encoding on the categorical columns for feature engineering

encoder = ce.BinaryEncoder(cols=['Customer_Category', 'Cust_State', 'Order_zone', 'CategoryName',
       'Transcation', 'Dept_Name', 'Product_type', 'Shipping_Class',
       'Warehouse_Region'])

df_train_encoded = encoder.fit_transform(df_train_copy)

df_train_encoded.head()

#same on test data

df_test_copy.select_dtypes(include='object').columns

test_encoder = ce.BinaryEncoder(cols=['Customer_Category', 'Cust_State', 'Order_zone', 'CategoryName',
       'Transcation', 'Dept_Name', 'Product_type', 'Shipping_Class',
       'Warehouse_Region'])

df_test_encoded = encoder.fit_transform(df_test_copy)

df_test_encoded.head()

df_test_copy.head()

#We have done the encoding using binary encoding technique on both the train and test data

"""We would use the df_train_encoded and df_test_encoded for further use

Model building
"""

df_train_encoded.columns

X = df_train_encoded.drop('Delivery_Status', axis=1)
X.columns

y = df_train_encoded['Delivery_Status']
y

#train test split on train data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.shape)
print(X_val.shape)
print(y_train.shape)
print(y_val.shape)

sel = SelectFromModel(RandomForestClassifier(n_estimators = 50))
sel.fit(X_train, y_train)

selected_feat= X_train.columns[(sel.get_support())]
print(selected_feat)

rfc = RandomForestClassifier(class_weight='balanced')
rfc.fit(X_train, y_train)

plt.figure(figsize=(15,8))
plt.barh(X_train.columns, rfc.feature_importances_)
plt.tight_layout()
plt.show()

#selecting the best features as per rfc feat importances
new_X = df_train_encoded[['Sales', 'Price', 'Shipping_Class_0', 'Zipcode','Order_Profit ', 'ProductLength', 'ProductWeight','Shipping_Class_2',
                          'Scheduled_Shipping']]

new_X.head()

df_test_copy = df_test_encoded[['Sales', 'Price', 'Shipping_Class_0', 'Zipcode','Order_Profit ', 'ProductLength', 'ProductWeight','Shipping_Class_2',
                          'Scheduled_Shipping']]

df_test_copy.head()

#again do the train test split for best features
X_train, X_val, y_train, y_val = train_test_split(new_X, y, test_size=0.2, random_state=42)

y

print(X_train.shape)
print(X_val.shape)
print(y_train.shape)
print(y_val.shape)

#use RFC to get the results on train data

rfc = RandomForestClassifier(n_estimators=50, max_depth=10, class_weight='balanced', random_state=42)
rfc.fit(X_train, y_train)

y_pred_train_rfc = rfc.predict(X_train)

print(confusion_matrix(y_train, y_pred_train_rfc))

print(classification_report(y_train, y_pred_train_rfc))

y_pred_val_rfc = rfc.predict(X_val)

print(confusion_matrix(y_val, y_pred_val_rfc))

#print the f1 score
print(f1_score(y_val, y_pred_val_rfc))

print(classification_report(y_val, y_pred_val_rfc))

#define the paramaters for rfc
params = {'criterion': ['gini','entropy'], 'max_depth': range(0,21), 'n_estimators': range(0,101), 'class_weight':['balanced', 'balanced_subsample']}

#use randomized search cv for parameter tuning
from sklearn.model_selection import KFold, RandomizedSearchCV, GridSearchCV

kf = KFold(n_splits=3, shuffle=True, random_state=42)

rscv = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42), 
                          param_distributions=params, n_iter=5, scoring='f1', n_jobs=-1, cv=kf, verbose=3, random_state=42)

rscv.fit(X_train, y_train)
rscv_tuned_rf = rscv.best_estimator_
print(rscv_tuned_rf)

scores = cross_val_score(rscv_tuned_rf, X_train, y_train, cv = kf, scoring = 'f1', n_jobs = -1)
print("RFC model tuned f1 scores: ", scores)
print("RFC model Bias Error: ", 1 - np.mean(scores))
print("RFC model Variance Error: ", np.std(scores, ddof = 1))

y_pred_val_rfc = rscv_tuned_rf.predict(X_val)

print(f1_score(y_val, y_pred_val_rfc))

#lets get our predictions using xgboost classifier
xgb = XGBClassifier(random_state=42)

xgb.fit(X_train, y_train)

y_pred_train_xgb = xgb.predict(X_train)

print(confusion_matrix(y_train, y_pred_train_xgb))

print(classification_report(y_train, y_pred_train_xgb))

y_pred_val_xgb = xgb.predict(X_val)

print(confusion_matrix(y_val, y_pred_val_xgb))

print(classification_report(y_val, y_pred_val_xgb))

print(f1_score(y_val, y_pred_val_xgb))

#Xgboost hyperparmeter tuning with rscv

params = {'max_depth':range(0,51), 'learning_rate':[0.01,0.05,0.1,0.5], 'n_estimators':range(0,101)}

rscv_xg = RandomizedSearchCV(estimator=XGBClassifier(random_state=42), param_distributions=params, n_iter=5, scoring='f1', n_jobs=-1, cv=kf, 
                             verbose=3, random_state=42)
rscv_xg.fit(X_train, y_train)
rscv_tuned_xg = rscv_xg.best_estimator_
print(rscv_tuned_xg)

scores = cross_val_score(rscv_tuned_xg, X_train, y_train, cv = kf, scoring = 'f1', n_jobs = -1)
print("XGB model tuned f1 scores: ", scores)
print("XGB model Bias Error: ", 1 - np.mean(scores))
print("XGB model Variance Error: ", np.std(scores, ddof = 1))

y_pred_train_xgb = rscv_tuned_xg.predict(X_val)

print(confusion_matrix(y_val, y_pred_val_xgb))

print(classification_report(y_val, y_pred_val_xgb))

#Adaboost model
ab = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10, class_weight='balanced'), 
                        n_estimators=50, learning_rate=0.1, random_state=42)

ab.fit(X_train, y_train)

y_pred_train_ab = ab.predict(X_train)

print(confusion_matrix(y_train, y_pred_train_ab))

print(classification_report(y_train, y_pred_train_ab))

y_pred_val_ab = ab.predict(X_val)

print(f1_score(y_val, y_pred_val_ab))

print(confusion_matrix(y_val, y_pred_val_ab))

print(classification_report(y_val, y_pred_val_ab))

import lightgbm
from lightgbm import LGBMClassifier

lgbm = LGBMClassifier(max_depth=7, n_estimators=87, class_weight='balanced', random_state=42, n_jobs=-1)

lgbm.fit(X_train, y_train)

y_pred_train_lgbm = lgbm.predict(X_train)

print(confusion_matrix(y_train, y_pred_train_lgbm))

print(classification_report(y_train, y_pred_train_lgbm))

y_pred_val_lgbm = lgbm.predict(X_val)

print(f1_score(y_val, y_pred_val_lgbm))

#install catboost

pip install catboost

from catboost import CatBoostClassifier

catboost = CatBoostClassifier(iterations=7, learning_rate=0.1, depth=8)

catboost.fit(X_train, y_train)

y_pred_train_cat = catboost.predict(X_train)

print(confusion_matrix(y_train, y_pred_train_cat))

print(classification_report(y_train, y_pred_train_cat))

#get predictions on the test set
final_predictions_ab = rfc.predict(df_test_copy)

#get order_id for test data
test_file_ids = df_test['OrderId']
test_file_ids

#create submission df
submission_rfc=pd.DataFrame([test_file_ids,final_predictions_ab]).T
submission_rfc.head()

submission_rfc.rename(columns={"Unnamed 0": "Delivery_Status"}, inplace=True)
submission_rfc.head()

submission_rfc.shape

#from google.colab import files
#drive.mount('/drive')

submission_rfc.to_csv('submission_rfc.csv',index = False)

from google.colab import files
files.download("submission_rfc.csv")

















